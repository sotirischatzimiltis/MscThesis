{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-x_8QuklY_q"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sotirischatzimiltis/MscThesis/blob/main/SpikingNeuralNetwork/spiking_neural_network_muliclass_classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v57CWggfcug0"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g8rLgoGUc-VW"
      },
      "outputs": [],
      "source": [
        "train_Dataset = 'train_rf05_multiclass.csv'\n",
        "test_Dataset = 'test_rf05_multiclass.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07CV2e3xc7d9"
      },
      "source": [
        "# Set Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J3OtoNSTc9rF"
      },
      "outputs": [],
      "source": [
        "bsize =1024 # set batch size \n",
        "learning_rate = 0.0005 # set learning rate of optimizer  \n",
        "num_epochs = 2 # number of epochs\n",
        "num_hidden = 200 # hidden layer neurons \n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 25\n",
        "beta = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tn_wUlopkon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c33083-df9f-4b0b-89c4-5e7cc087d9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.12.0+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->snntorch) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2022.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXZ6Tuqc9Q-l"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJdfllYbc16"
      },
      "source": [
        "# 1. Setting up the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTsa-qvjoWtl"
      },
      "source": [
        "Get data from GitLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guduK5-i9SGK"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self,file_name):\n",
        "    df=pd.read_csv(file_name)\n",
        "\n",
        "    x=df.iloc[:,:-1].values\n",
        "    y=df.iloc[:,-1].values\n",
        "    print(x)\n",
        "    print(y)\n",
        "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
        "    self.y_train=torch.tensor(y,dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx],self.y_train[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G4mxwzJ7tbS"
      },
      "outputs": [],
      "source": [
        "batch_size = bsize\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "trainDs=MyDataset(train_Dataset)\n",
        "testDs=MyDataset(test_Dataset)\n",
        "\n",
        "train_loader = DataLoader(trainDs, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(testDs, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhFyzySNeT_e"
      },
      "source": [
        "# 2. Define the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lud3kywn55fj"
      },
      "outputs": [],
      "source": [
        "# Network Architecture\n",
        "num_inputs = len(trainDs.x_train[1]) # input layer neurons i\n",
        "num_outputs = 5 # output layer neurons  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uquHLLmpkox"
      },
      "outputs": [],
      "source": [
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "        self.fc3 = nn.Linear(num_hidden,num_outputs)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        \n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3,mem3)\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "        \n",
        "# Load the network onto CUDA if available\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0fHcAKfrav6"
      },
      "source": [
        "The code in the `forward()` function will only be called once the input argument `x` is explicitly passed into `net`.\n",
        "\n",
        "* `fc1` applies a linear transformation to all input pixels from the MNIST dataset;\n",
        "* `lif1` integrates the weighted input over time, emitting a spike if the threshold condition is met;\n",
        "* `fc2` applies a linear transformation to the output spikes of `lif1`;\n",
        "* `lif2` is another spiking neuron layer, integrating the weighted spikes over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7MdORCtIx4"
      },
      "source": [
        "# 3. Training the SNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-fhT3Q7nXM"
      },
      "source": [
        "## 3.1 Accuracy Metric\n",
        "Below is a function that takes a batch of data, counts up all the spikes from each neuron (i.e., a rate code over the simulation time), and compares the index of the highest count with the actual target. If they match, then the network correctly predicted the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IxcnBAxpkoy"
      },
      "outputs": [],
      "source": [
        "# pass data into the network, sum the spikes over time\n",
        "# and compare the neuron with the highest number of spikes\n",
        "# with the target\n",
        "\n",
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    output, _ = net(data.view(batch_size, -1))\n",
        "    ######check#########\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
        "    print_batch_accuracy(data, targets, train=True)\n",
        "    print_batch_accuracy(test_data, test_targets, train=False)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woJSGSx68tsd"
      },
      "source": [
        "## 3.2 Loss Definition\n",
        "The `nn.CrossEntropyLoss` function in PyTorch automatically handles taking the softmax of the output layer as well as generating a loss at the output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqdVyjCNtdlp"
      },
      "outputs": [],
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fPgSoO9Jgb"
      },
      "source": [
        "## 3.3 Optimizer\n",
        "Adam is a robust optimizer that performs well on recurrent networks, so let's use that with a learning rate of $5\\times10^{-4}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l62ZR51s9Lxg"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVgKDes8BiXq"
      },
      "source": [
        "## 3.4 Training Loop\n",
        "\n",
        "Let's combine everything into a training loop. We will train for one epoch (though feel free to increase `num_epochs`), exposing our network to each sample of data once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMZMxEV8dcTC"
      },
      "outputs": [],
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "trainloss = np.zeros(num_epochs)\n",
        "testloss = np.zeros(num_epochs)\n",
        "trainacc = np.zeros(num_epochs)\n",
        "testacc = np.zeros(num_epochs)\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    teloss = 0.0\n",
        "    train_acc =0.0\n",
        "    test_acc = 0.0\n",
        "    iter_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "    train_samples = 0\n",
        "    test_samples = 0\n",
        "    # Minibatch training loop\n",
        "    for data, targets in train_batch:\n",
        "        train_samples += 1\n",
        "        #spike_data = spikegen.rate(data, num_steps=num_steps)\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        net.train()\n",
        "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
        "        _, idx = spk_rec.sum(dim=0).max(1)\n",
        "        acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "        train_acc+= acc\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        for step in range(num_steps):\n",
        "            loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "        train_loss += loss_val.item()\n",
        "        # Test set\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            test_data, test_targets = next(iter(test_loader))\n",
        "            #spike_test_data = spikegen.rate(test_data,num_steps =num_steps)\n",
        "            test_data = test_data.to(device)\n",
        "            test_targets = test_targets.to(device)\n",
        "            test_samples += 1\n",
        "\n",
        "            # Test set forward pass\n",
        "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
        "            _, idx = test_spk.sum(dim=0).max(1)\n",
        "            tacc = np.mean((test_targets == idx).detach().cpu().numpy())\n",
        "            test_acc += tacc\n",
        "            # Test set loss\n",
        "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "                test_loss += loss(test_mem[step], test_targets)\n",
        "            test_loss_hist.append(test_loss.item())\n",
        "            teloss += test_loss.item()\n",
        "            # Print train/test loss/accuracy\n",
        "            if counter % 50 == 0:\n",
        "                train_printer()\n",
        "            counter += 1\n",
        "            iter_counter +=1\n",
        "    trainloss[epoch] = train_loss \n",
        "    testloss[epoch] = teloss\n",
        "    trainacc[epoch] = train_acc/train_samples\n",
        "    testacc[epoch] = test_acc/ test_samples\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HzVmh7s2kw3"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(trainloss)):\n",
        "  print(trainloss[i])\n",
        "\n",
        "print(\"test--------------------------------------------------------------------\")\n",
        "\n",
        "for i in range(0,len(testloss)):\n",
        "  print(testloss[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCdDeH9GqUPj"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(trainloss)\n",
        "plt.plot(testloss)\n",
        "plt.title(\"Train & Test Loss Curves\")\n",
        "plt.legend([\"Train Loss\",\"Test Loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5cEQR1zYD4G"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(trainacc)\n",
        "plt.plot(testacc)\n",
        "plt.title(\"Train & Test Accuracy Curves\")\n",
        "plt.legend([\"Train Accuracy\",\"Test Accuracy\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accurracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HxU7P7xFpko3"
      },
      "source": [
        "# 4. Results\n",
        "## 4.1 Plot Training/Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pk_EScnpkpj"
      },
      "outputs": [],
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.title(\"Loss Curves\")\n",
        "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Gd84OAl1rB"
      },
      "source": [
        "The loss curves are noisy because the losses are tracked at every iteration, rather than averaging across multiple iterations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Z3f0vBnBpkpk"
      },
      "source": [
        "## 4.2 Test Set Accuracy\n",
        "This function iterates over all minibatches to obtain a measure of accuracy over the full samples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5Rb4xHGndQh"
      },
      "outputs": [],
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "y_pred = [] # for confusion matrix \n",
        "y_true = [] # for confusion matrix \n",
        "# drop_last switched to False to keep all samples\n",
        "test_loader = DataLoader(testDs, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad(): # switch off gradient computation\n",
        "  net.eval() # switch for evaluation \n",
        "  for data, targets in test_loader: # load batch of data and targets\n",
        "    data = data.to(device) # sent to gpu\n",
        "    targets = targets.to(device) # sent to gpu \n",
        "    \n",
        "    # forward pass\n",
        "    test_spk, _ = net(data.view(data.size(0), -1)) # forward pass to predict\n",
        "\n",
        "    # calculate total accuracy\n",
        "    _, predicted = test_spk.sum(dim=0).max(1)\n",
        "    y_pred.extend(predicted.cpu()) # Save Prediction\n",
        "    y_true.extend(targets.cpu())\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets).sum().item()\n",
        "    \n",
        "print(f\"Total correctly classified instances: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# constant for classes\n",
        "classes = ('Normal', 'DoS','Probe','R2L','U2R')\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# # rows is the truth , columns is the predicted \n",
        "# plot confusion matrix with actual numbers\n",
        "df_cm= pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True,fmt ='g')\n",
        "\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_true,y_pred,target_names = classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dv7p4FjTK1f"
      },
      "outputs": [],
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "y_pred = [] # for confusion matrix \n",
        "y_true = [] # for confusion matrix \n",
        "# drop_last switched to False to keep all samples\n",
        "train_loader = DataLoader(trainDs, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad(): # switch off gradient computation\n",
        "  net.eval() # switch for evaluation \n",
        "  for data, targets in train_loader: # load batch of data and targets\n",
        "    data = data.to(device) # sent to gpu\n",
        "    targets = targets.to(device) # sent to gpu \n",
        "    \n",
        "    # forward pass\n",
        "    train_spk, _ = net(data.view(data.size(0), -1)) # forward pass to predict \n",
        "\n",
        "    # calculate total accuracy\n",
        "    _, predicted = train_spk.sum(dim=0).max(1)\n",
        "    y_pred.extend(predicted.cpu()) # Save Prediction\n",
        "    y_true.extend(targets.cpu())\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified instances: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# constant for classes\n",
        "classes = ('Normal', 'DoS','Probe','R2L','U2R')\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "# rows is the truth , columns is the predicted \n",
        "# plot confusion matrix with actual numbers\n",
        "df_cm= pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True,fmt ='g')\n",
        "\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_true,y_pred,target_names = classes))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "spiking_neural_network_muliclass_classification .ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}